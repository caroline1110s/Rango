{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSys 2020 - Lab 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caroline1110s/Rango/blob/master/RecSys_2020_Lab_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Inwi1o3992TW",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2\n",
        "\n",
        "The aims of this lab are to:\n",
        " - Make your first recommendations using Spotlight recommender toolkit on explicit data\n",
        " - Start to think about explicit vs implicit learners\n",
        " - Evaluate your results using Spotlight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys992pU79yhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Standard setup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0oobMGk5eqv",
        "colab_type": "text"
      },
      "source": [
        "We're going to use the Spotlight library - see https://github.com/maciejkula/spotlight - and its documentation at https://maciejkula.github.io/spotlight/\n",
        "\n",
        "We can install this direct from Git, but using Craig's patched version.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDObURPI-Shz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Vlbi9W-d2j",
        "colab_type": "text"
      },
      "source": [
        "We'll be using Movielens again. Lets load it in to the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTHXCky5-i_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -o ml-latest-small.zip http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip -o ml-latest-small.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt-0EHa7-sWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ratings_df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
        "movies_df = pd.read_csv(\"ml-latest-small/movies.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN_OpdQYHmTb",
        "colab_type": "text"
      },
      "source": [
        "#Part 1 - Getting (Explicit) Rating Data into Spotlight and Analysing Outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqoiteq7IJzJ",
        "colab_type": "text"
      },
      "source": [
        "Now we can get onto some real recommendation work. Spotlight has a handy [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object, which encapsulates the basics of a recommendations dataset.\n",
        "\n",
        "In fact, there are handy loaders for a few standard datasets including MovieLens, but lets make our own, so that we can match back to the dataframe.\n",
        "\n",
        "Interactions needs numbers as userids and itemids. Unfortunately, our MovieLens uses numbers, but these aren't consecutive (i.e. we have missing movieIds values). So, for both movies and users, we will use [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) to convert the MovieLens integers down to consecutive integers for use in Spotlight, in the `uid_map` and `iid_map` objects. We'll keep the reverse mapping around too, in case we want to lookup the actual movieId given the uid recorded by SpotlightÂ (etc). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF89PzxNHrHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "uid_map = defaultdict(count().__next__)\n",
        "iid_map = defaultdict(count().__next__)\n",
        "uids = np.array([uid_map[uid] for uid in ratings_df[\"userId\"].values ], dtype=np.int32)\n",
        "iids = np.array([iid_map[iid] for iid in ratings_df[\"movieId\"].values ], dtype=np.int32)\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "\n",
        "ratings = ratings_df[\"rating\"].values.astype(np.float32)\n",
        "timestamps = ratings_df[\"timestamp\"].values.astype(np.int32)\n",
        "\n",
        "print(\"userId %d got uid %d\" % (556, uid_map[556]))\n",
        "print(\"movieId %d got iid %d\" % (54001, iid_map[54001]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMejAIwPNVrU",
        "colab_type": "text"
      },
      "source": [
        "Now lets built an [Interactions](https://maciejkula.github.io/spotlight/interactions.html) object from Spotlight. This contains everything that Spotlight needs to train a model. We can split it up randomly into train and test subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg2tNWwPIBfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "dataset = Interactions(user_ids=uids,\n",
        "                                  item_ids=iids,\n",
        "                                  ratings=ratings,\n",
        "                                  timestamps=timestamps)\n",
        "\n",
        "#lets initialise the seed, so that its repeatable and reproducible \n",
        "train, test = random_train_test_split(dataset, random_state=np.random.seed(42))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1shoeRmWKXxS",
        "colab_type": "text"
      },
      "source": [
        "Lets see how big the two datasets are. What is the train/test split percentage size?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcjOWJ-qIEge",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "print(train)\n",
        "print(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTyhLswNulTX",
        "colab_type": "text"
      },
      "source": [
        "Now, you can think of the Interactions objects are being the partitions of the rating matrix. But we dont store it as a single big matrix. Instead, we record three one-dimensional arrays:\n",
        " \n",
        "  * one for the ids of the users\n",
        "  * one for the ids of the items\n",
        "  * one for the actual rating values.\n",
        "\n",
        "Each of these arrays is the size of the number of ratings.\n",
        "\n",
        "Lets take our favourite fantasy adventure fan, userId 556. We can give a look at their training ratings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47QmgWtYvM4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#map userId to the internal uid value\n",
        "userId = 556\n",
        "uid = uid_map.get(556)\n",
        "\n",
        "#see which ratings are for this user. Use this to filter the item and ratings arrays\n",
        "print(train.item_ids[train.user_ids == uid])\n",
        "print(train.ratings[train.user_ids == uid])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnKAa-KKclT",
        "colab_type": "text"
      },
      "source": [
        "Right, now we can learn a model. Lets start with a matrix factorisation for explicit data.  We train the model using the `fit` method. This is just like the `fit` in Sklearn - we're fitting  a model to the specified training data.\n",
        "\n",
        "This might take upto a minute. \n",
        "\n",
        "**NB:**  that Spotlight can support using GPUs which we could use to slightly speed up training time, that but will make our life more difficult later on, so lets ignore this for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UduCmnlbKt-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "emodel = ExplicitFactorizationModel(n_iter=10,\n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False)\n",
        "current = time.time()\n",
        "\n",
        "emodel.fit(train, verbose=True)\n",
        "\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds \"% (diff))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTK1BlbLL_t",
        "colab_type": "text"
      },
      "source": [
        "How well did we do. Well, lets give a look at the recommentations, for our specific user, userId 556. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjyVPjoGLoy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"One test item_id for userId 556 (uid %d) is \" % uid_map.get(556))\n",
        "\n",
        "testItemId = test.item_ids[np.where(test.user_ids == uid_map.get(556))[0][0]]\n",
        "print(\"Test movieId is %d itemId %d \" % (iid_rev_map.get(testItemId), testItemId ) )\n",
        "\n",
        "\n",
        "#here 0 is a dummy item, which Spotlight needs for some reason...\n",
        "#we discard its prediction using [1]\n",
        "predicted = emodel.predict( np.array([555]), item_ids=np.array([0, testItemId]))[1]\n",
        "\n",
        "#what was the actual score of the user for that movie?\n",
        "#we can get the appropriate row from the ratings dataframe, then extract that value\n",
        "actual = ratings_df[(ratings_df.movieId==iid_rev_map.get(testItemId)) & (ratings_df.userId==556)][[\"rating\"]].values[0][0]\n",
        "\n",
        "print(\"Predicted rating was %f, actual rating %0.1f, error was %f\" % (predicted, actual, abs(predicted-actual) )) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgX01Wwlr_WA",
        "colab_type": "text"
      },
      "source": [
        "We can also ask for **all** of the recommendations for a given user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz28wrmIsDa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allpreds = emodel.predict( np.array([555]) )\n",
        "\n",
        "print(allpreds)\n",
        "print(allpreds.size)\n",
        "\n",
        "#we can recover the original rating for movieId 48394\n",
        "print(allpreds[1132])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyL5EG65TuNo",
        "colab_type": "text"
      },
      "source": [
        "Lets see how these recommendations are made. Remember from Lecture 9 that the prediction is made based on the dot product of the user's and item's latent factors.\n",
        "\n",
        "We can access these embeddings directly from the emodel object. Each embedding has 32 dimensions, which is what we set when configuring Spotlight's Explicit Factorisation Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf2Em9KSa74G",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#these two statements are equivalent\n",
        "#emodel._net.item_embeddings(torch.tensor([0]))\n",
        "\n",
        "emodel._net.item_embeddings.weight[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKlJIBoVNYr",
        "colab_type": "text"
      },
      "source": [
        "We can check how Spotlight makes its prediction. The key line is https://github.com/maciejkula/spotlight/blob/master/spotlight/factorization/representations.py#L89\n",
        "\n",
        "This takes the (dot-)product of the user's \"embedding\" (latent factor) and the item's embedding. On top of these are added \"user_biases\" and \"item_biases\". What do you think these last two components are for?\n",
        "\n",
        "Lets reproduce this for our favourite user..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g14v62m4YRyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(\n",
        "  (emodel._net.user_embeddings.weight[555] * emodel._net.item_embeddings.weight[1893]).sum(0) \n",
        "    + emodel._net.user_biases(torch.tensor([555]))  \n",
        "    + emodel._net.item_biases(torch.tensor([1893]))\n",
        "\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VE2tpcJb8e7",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "Lets give a look at item-item similarities. Write a function `mostsimilar(targetMovieId, model):` that identifies the most similar movieId to the specified target, based on the Cosine similarity of their item embedding vectors. \n",
        "\n",
        "What's the closest movie to \"Harry Potter and the Deathly Hallows: Part 1 (2010)\" , which is movieId 81834 in the MovieLens dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BC95DAUcu45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def mostsimilar(targetMovieId, model):\n",
        "  highest=0\n",
        "  highestCos=0\n",
        "  targetIId=iid_map.get(targetMovieId)\n",
        "  \n",
        "  #you may assume that model._num_items items.\n",
        "  #nn.functional.cosine_similarity() allows you to calculate the cosine similarity between two vectors.\n",
        "  \n",
        "  \n",
        "  ##SOLUTION FROM HERE\n",
        "  \n",
        "\n",
        "  print(train.num_items)\n",
        "  print(\"targetMovieId = %d (iid %d)\" % (targetMovieId, targetIId))\n",
        "  print(\"mostSimilar = %d (iid %d) with cosine of %f \" % ( iid_rev_map.get(highest), highest, highestCos))\n",
        "  \n",
        "  \n",
        "mostsimilar(81834, emodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxHfMZgbcdRu",
        "colab_type": "text"
      },
      "source": [
        "Finally, lets see how good we are at our rating predictions. Handily, Spotlight implements a few common evaluation measures for us to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB8UJykycm3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.evaluation import rmse_score\n",
        "\n",
        "train_rmse = rmse_score(emodel, train)\n",
        "test_rmse = rmse_score(emodel, test)\n",
        "\n",
        "print('Train RMSE {:.3f}, test RMSE {:.3f}'.format(train_rmse, test_rmse))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhysLfp2dA28",
        "colab_type": "text"
      },
      "source": [
        "**Additional Exercise**:\n",
        "Change one of the following parameters - does this make matrix factorisation more or less accurate at rating prediction?:\n",
        "\n",
        "1. Increase the number of latent factors from 32 to 128\n",
        "2. Decrease the number of training iterations.\n",
        "3. Use `spotlight.cross_validation.user_based_train_test_split()` to split the data instead of a random split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uuzqWhOGhRW",
        "colab_type": "text"
      },
      "source": [
        "We can also project the item embeddings down into 2 dimensions using PCA (principal component analysis), for plotting on a figure (I followed the guidance at https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html). From the random selection of movies shown, are there discernable pattern in the points in this projection? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cijXx_C77L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "latent2D = pca.fit(emodel._net.item_embeddings.weight.detach().numpy()).transform(emodel._net.item_embeddings.weight.detach().numpy())\n",
        "\n",
        "print('explained variance ratio (first two components): %s'\n",
        "      % str(pca.explained_variance_ratio_))\n",
        "\n",
        "\n",
        "for i, title in enumerate(movies_df[\"title\"]):\n",
        "    if random.randint(1,1001) > 999:\n",
        "      plt.scatter(latent2D[i,0], latent2D[i,1])\n",
        "      plt.text(latent2D[i,0], latent2D[i,1], title, fontsize=9, rotation=45)\n",
        " \n",
        "plt.show\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aCiYTWaeobQ",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 - Implicit Recommendation\n",
        "\n",
        "This part of the lab uses a music dataset from [Last.fm](https://www.last.fm/) -- a Spotify-like music streaming service -- that was obtained by a researcher at Pompeu Fabra University (Barcelona, Spain). The relevant citation is:\n",
        "\n",
        "```\n",
        "  @book{Celma:Springer2010,\n",
        "      \tauthor = {Celma, O.},\n",
        "      \ttitle = {{Music Recommendation and Discovery in the Long Tail}},\n",
        "       \tpublisher = {Springer},\n",
        "       \tyear = {2010}\n",
        "      }\n",
        " ```\n",
        "\n",
        "You see more information about the dataset at [this link](https://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDavoa3qC64",
        "colab_type": "text"
      },
      "source": [
        "## Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-drWmULel_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf lastfm-dataset-1K.tar.gz\n",
        "!curl -o \"lastfm-dataset-1K.tar.gz\" \"http://www.dcs.gla.ac.uk/~craigm/recsysH/lastfm-dataset-1K.tar.gz\"\n",
        "!tar -zxvf lastfm-dataset-1K.tar.gz\n",
        "!ls -lh lastfm-dataset-1K/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcxILh_-h773",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listens_df = pd.read_csv(\"lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\",  names=['user', 'timestamp', 'artistid', 'artist', 'trackid', 'trackname'], header=None, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSwFnZSgrNU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Lets look at the dataset. Note that the we dont have any explicit ratings by the users. We just know what they interacted with (and when). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7RxgUI9gtu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listens_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrNF0jfAkZ3U",
        "colab_type": "text"
      },
      "source": [
        "Actually, these artist and trackids are from [Musicbrainz](https://musicbrainz.org/), and not all artists/tracks have entries. \n",
        "\n",
        "***Exercise***\n",
        "Count how many unique tracks do not have a musicbrainz trackid. Hint: You can use `len()` on a `groupby()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xflibwGF-TKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGzG9Rig3E6",
        "colab_type": "text"
      },
      "source": [
        "## An implicit recommendation approach\n",
        "\n",
        "Lets move away from explicit recommendation to implicit.\n",
        "\n",
        "We will continue using the [Spotlight](https://github.com/maciejkula/spotlight/) toolkit for our recommender. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U03zZ6CH---1",
        "colab_type": "text"
      },
      "source": [
        "We can construct [Interaction](https://maciejkula.github.io/spotlight/interactions.html) objects for Spotlight in the same way as before. The only difference is that this time we do not record the user's ratings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnovh7XmoFdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Some tracks dont seem to have artists or track names, so lets drop them for simplicity.\n",
        "listens_df = listens_df[listens_df.artist.notnull()]\n",
        "listens_df = listens_df[listens_df.trackname.notnull()]\n",
        "\n",
        "#the dataframe is VERY big (19M interactions), so lets just work with a small sample of it (this will mean that effectiveness will be lower, but learning will be MUCH faster).\n",
        "listens_df = listens_df.sample(n=200000, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcRhNWXzg7LT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from itertools import count\n",
        "\n",
        "#we cant trust the musicbrainz ids to exist, so lets build items ids based on artist & trackname attributes\n",
        "iid_map = defaultdict(count().__next__)\n",
        "iids = np.array([iid_map[artist+\"/\"+trackname] for artist,trackname in listens_df[[\"artist\",\"trackname\"]].values ], dtype=np.int32)\n",
        "\n",
        "\n",
        "uid_map = defaultdict(count().__next__)\n",
        "uids = np.array([uid_map[uid] for uid in listens_df[\"user\"].values ], dtype=np.int32)\n",
        "\n",
        "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
        "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
        "\n",
        "from spotlight.interactions import Interactions\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "#NB: we will set num_users and num_items here - its a good practice.\n",
        "imp_dataset = Interactions(user_ids=uids, item_ids=iids, num_users=len(uid_map), num_items=len(iid_rev_map))\n",
        "#we could add the timestamps here if we were doing sequence recommendation\n",
        "\n",
        "#what have we got.\n",
        "print(imp_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22dmr7JKqUnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.cross_validation import random_train_test_split\n",
        "\n",
        "itrain, itest = random_train_test_split(imp_dataset, random_state=np.random.seed(42))\n",
        "print(itrain)\n",
        "print(itest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFQazPPxhG2_",
        "colab_type": "text"
      },
      "source": [
        "Let's run Spotlight's a impllicit Matrix Factorisation on this dataset. Here, we use a *pointwise* loss, which just tries to predict the item or not.\n",
        "\n",
        "**Warning** this dataset is difficult for the learner - this *will* take a few minutes to learn... Use the time to read-on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3ZwYgkhKvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
        "import time  \n",
        "\n",
        "imodel = ImplicitFactorizationModel(n_iter=5, \n",
        "                                    embedding_dim=32, #this is Spotlight default\n",
        "                                    use_cuda=False)\n",
        "current = time.time()\n",
        "\n",
        "imodel.fit(itrain, verbose=True)\n",
        "end = time.time()\n",
        "diff = end - current\n",
        "print(\"Training took %d seconds\" % (diff))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zso4C5wehLog",
        "colab_type": "text"
      },
      "source": [
        "Again, we can look at the predictions. We make a prediction (a score ) for ALL items. Note that the scores vary in magnitude - indeed, we're not predicting a rating, we just need to have scores in order to rank the in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR_qbWXEhUDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(imodel.predict(0))\n",
        "print(len(imodel.predict(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwsF85t1MWJ-",
        "colab_type": "text"
      },
      "source": [
        "Indeed, lets give a look at the distribution of scores. Its a fairly 'normal' distribution - there are very few items with high scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFaTJIW2MThw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.hist(imodel.predict(0), color = 'blue', edgecolor = 'black',\n",
        "         bins = 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyZ3PyAxhdDF",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the scores of all items for a given user, we need to identify the top-scored ones, i.e. those that we would present to the user. \n",
        "\n",
        "**Exercise**: Write a function `tracksForUser(user)` to identify the artist name & track of the top K (e.g. K=4) items based on their score for a given user index index (i.e. 0.. 964). Hint: Try SciPy's [rankdata](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html) function. I also found [np.argwhere](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argwhere.html) to be userful. Alternatively, you can sort and then slice.\n",
        "\n",
        "What tracks are recommended for user uid 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBGdB7SOhcjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWFSAuQ40p2Y",
        "colab_type": "text"
      },
      "source": [
        "**Exercise** Look at the artists actually listened to by uid 10, and compare/contrast with predictions. Hint use a groupby on a suitable subset of the listens_df dataframe. If you can, sort by descending frequency of listen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnNErTHP0u8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2FWymqQRxKj",
        "colab_type": "text"
      },
      "source": [
        "I observed that uid 10 listened frequently to \"The Smashing Pumpkins\" (rank 1)  and \"R.E.M.\" (rank 7), which arent too different to suggestions of \"Radiohead\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTNae6Romuk",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating an implicit recommender\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Q2TTpZuHON",
        "colab_type": "text"
      },
      "source": [
        "We can examine the MRR of the implicit model we have learned. We pass it the test set (which contains knowledge of what the user *actually* clicked), as our ground truth. \n",
        "\n",
        "In the second variant, we also pass the training data. This prevents the recommender gaining credit for recommending items that were part of the training data for that user.\n",
        "\n",
        "You can see the implementation of [mrr_score()](https://github.com/cmacdonald/spotlight/blob/master/spotlight/evaluation.py#L8).\n",
        "\n",
        "**Questions for you to consider**\n",
        " - Why is the second score lower? \n",
        " - Would this be the same for all recommendation settings? \n",
        " - In the implementation, why are the scores negated, why do we use [rankdata()](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html)?\n",
        " \n",
        "We will use the first variant for this Lab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLV71LSjt-k2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spotlight.evaluation import mrr_score\n",
        "\n",
        "#evaluate on this dataset takes approx 1 minute\n",
        "!date\n",
        "print(mrr_score(imodel, itest).mean())\n",
        "!date\n",
        "print(mrr_score(imodel, itest,  train=itrain).mean())\n",
        "!date\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjM_kZNtAE2k",
        "colab_type": "text"
      },
      "source": [
        "How to interpret an MRR score - we know it has a range [0,1] with 1 being best. 1 means, on average across all users, we make a relevant prediction at rank 1; 0.5 means, on average, at rank 2. This is a very rough rule-of-thumb - MRR isnt a linear measure, so  a few poor predictions affect the average more than a few good ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL2jRiGF2uLb",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "*   Pick a user with the highest RR. How many listens did they have in the training dataset?\n",
        "*   Similalry, pick a user with the lowest RR. How many listens did they have in the training dataset?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEliH7_O2-5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf48Wm55tJvJ",
        "colab_type": "text"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "Train an implicit model using the `loss=\"bpr\"` option of ImplicitFactorizationModel. \n",
        " \n",
        "*   The loss function values are smaller. What can we interpret from that?\n",
        "*   Compare its effectiveness. Is this result expected?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNeyn7o2tf54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awTPM_BGcUc0",
        "colab_type": "text"
      },
      "source": [
        "## End of lab\n",
        "\n",
        "After you have completed the lab, please complete the [Lab 2 feedback quiz](https://moodle.gla.ac.uk/mod/feedback/view.php?id=1444707) on the RecSys Moodle page.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}